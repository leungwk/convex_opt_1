; -*- mode: org;-*-

* about

Code for assignments and problem sets in Convex Optimization I ([[http://see.stanford.edu/see/courseinfo.aspx?coll=2db7ced4-39d1-4fdb-90e8-364129597c87][SEE version]]).

* convex opt 1
* grad

looks like when \beta=0.1, slow convergence. Why? Because if =t <-
t*beta=, then the step size does not update as quickly.

** future

- why does gradient descent converge so quickly?

